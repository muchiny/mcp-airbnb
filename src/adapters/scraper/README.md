# ğŸ•·ï¸ Web Scraper Adapter

Implements `AirbnbClient` by scraping public Airbnb pages. This is the primary data source for the MCP server â€” it fetches HTML pages, extracts structured JSON from embedded scripts, and falls back to CSS selectors when needed.

## ğŸ“‚ Files

| File | Responsibility |
|------|---------------|
| `client.rs` | `AirbnbScraper` struct â€” HTTP fetching, retry with exponential backoff, cache-aside pattern |
| `search_parser.rs` | Parses search results page â†’ `SearchResult` |
| `detail_parser.rs` | Parses listing detail page â†’ `ListingDetail` |
| `review_parser.rs` | Parses reviews from listing page â†’ `ReviewsPage` |
| `calendar_parser.rs` | Parses price calendar from listing page â†’ `PriceCalendar` |
| `rate_limiter.rs` | Tokio-compatible rate limiter with configurable interval |

## ğŸ”§ `AirbnbScraper`

The main client struct owns:

- **`reqwest::Client`** â€” HTTP client with cookie jar and custom User-Agent
- **`RateLimiter`** â€” Throttles requests to respect Airbnb's rate limits
- **`Arc<dyn ListingCache>`** â€” Shared cache reference for the cache-aside pattern
- **`ScraperConfig` + `CacheConfig`** â€” Runtime configuration

### Cache-Aside Pattern

Every `AirbnbClient` method follows the same flow:

1. Build cache key (e.g., `detail:{id}`)
2. Check cache â€” if hit, deserialize and return
3. Rate-limit, then fetch HTML via `fetch_html()`
4. Parse HTML with the appropriate parser
5. Serialize and store in cache with TTL
6. Return the parsed result

### Retry Logic

`fetch_html()` retries on failure with exponential backoff:

- Up to `max_retries` attempts (default: 2)
- Delay: `attempt * 2` seconds between retries
- Re-acquires rate limiter token before each retry
- Returns `RateLimited` error on HTTP 429
- Returns `Parse` error on HTTP 404 (no retry)

## ğŸ“Š Parser Architecture

Each parser follows a three-tier strategy to maximize extraction success:

```mermaid
sequenceDiagram
    participant Client as AirbnbScraper
    participant Cache as MemoryCache
    participant RL as RateLimiter
    participant HTTP as reqwest::Client
    participant Parser as Parser Module

    Client->>Cache: get(cache_key)
    alt Cache Hit
        Cache-->>Client: cached JSON
        Client->>Client: serde_json::from_str()
        Client-->>Client: Return result
    else Cache Miss
        Client->>RL: wait()
        RL-->>Client: Ready
        Client->>HTTP: GET url
        HTTP-->>Client: HTML response
        Client->>Parser: parse(html, ...)
        Note over Parser: 1. Try __NEXT_DATA__ JSON
        Note over Parser: 2. Try data-deferred-state JSON
        Note over Parser: 3. Fall back to CSS selectors
        Parser-->>Client: Parsed result
        Client->>Cache: set(key, json, TTL)
    end
```

### Parsing Tiers

1. **`__NEXT_DATA__`** â€” Airbnb embeds a `<script id="__NEXT_DATA__">` tag containing the full page data as JSON. This is the most reliable source.
2. **`data-deferred-state`** â€” Some pages use `<script>` tags with `data-deferred-state` attributes containing deferred JSON payloads.
3. **CSS Selectors** â€” Last resort fallback. Extracts data from HTML elements using `itemprop`, `data-testid`, and other attributes.

## â±ï¸ Rate Limiter

Token-bucket style limiter (`rate_limiter.rs`):

- Calculates `min_interval` from `rate_limit_per_second` (e.g., 0.5 req/s â†’ 2 second interval)
- Tracks last request time via `Mutex<Option<Instant>>`
- Calls `tokio::time::sleep()` when throttled â€” fully async-compatible
- Applied before every HTTP request, including retries
